\def\year{2017}\relax
\documentclass[letterpaper]{article}
\usepackage{aaai17}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}

% Packages added by Rob
% Also: grffile, latexsym, textcomp, longtable, booktabs, amsmath, amsfonts, amssymb?
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{graphicx}
\providecommand{\tightlist}{\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}%

% Packages added by Josh
\usepackage{amsmath}

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Insert Your Title Here)
/Author (Put All Your Authors Here, Separated by Commas)}
\setcounter{secnumdepth}{0}
\begin{document}

\title{ConceptNet 5.5: An Open Multilingual Knowledge Graph about Natural Language}
\author{Robert Speer\\ Luminoso Technologies, Inc.}

\maketitle


\section{Abstract}\label{abstract}

ConceptNet is a knowledge graph that connects words and phrases of
natural language with labeled edges. It is designed to improve natural
language applications by allowing the application to better understand
the meanings behind the words people use. Version 5.5 extends its
representation to include word forms in many languages.

ConceptNet provides applications with understanding that they would not acquire
from distributional semantics (such as word2vec) alone, nor from narrower
resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art
results on intrinsic evaluations (word relatedness and analogies) that
translate into improvements on an extrinsic evaluation in story understanding
(the Story Cloze Test).


\section{Introduction}\label{introduction}

ConceptNet is a knowledge graph that connects words and phrases of
natural language (\emph{terms}) with labeled, weighted edges
(\emph{assertions}). The original release of ConceptNet \cite{liu2004conceptnet}
was intended as a parsed representation of Open Mind Common Sense
\cite{singh2002omcs}, a crowd-sourced knowledge project. This paper
describes the release of ConceptNet 5.5, which has expanded to include
lexical and world knowledge from many different sources in many
languages.

[...]

In this paper, we will concisely represent an assertion as a triple of
its start node, relation label, and end node: the assertion that ``a dog
has a tail'' can be represented as (\emph{dog}, \emph{HasA},
\emph{tail}).


\section{Structure of ConceptNet}\label{structure-of-conceptnet}


\subsection{Knowledge sources}\label{knowledge-sources}

ConceptNet 5.5 is built from the following sources:

\begin{itemize}
\item
  Facts acquired from Open Mind Common Sense (OMCS) \cite{singh2002omcs}
\item
  Information extracted from parsing Wiktionary, in multiple languages,
  with a custom parser (``Wikiparsec'')
\item
  ``Games with a purpose'' designed to collect common knowledge,
  including Verbosity \cite{vonahn2006verbosity} in English, nadya.jp
  \cite{nakahara2011nadya} in Japanese, and the PTT Pet Game
  \cite{kuo2009petgame} in Chinese
\item
  Open Multilingual WordNet \cite{bond2013linking}, a linked-data
  representation of WordNet \cite{miller1998wordnet} and its parallel
  projects in multiple languages, representing word meanings as
  ``synsets'' of synonymous word senses
\item
  JMDict \cite{breen2004jmdict}, a Japanese-multilingual dictionary
\item
  OpenCyc \cite{matuszek2006cyc}, a hierarchy of hypernyms provided by
  Cyc, a system that represents common sense knowledge in predicate
  logic
\item
  A subset of DBPedia \cite{auer2007dbpedia}, a network of facts
  extracted from Wikipedia infoboxes
\end{itemize}

With the combination of these sources, ConceptNet contains over 21
million edges and over 8 million nodes. Its English vocabulary contains
approximately 1,500,000 nodes, and there are 83 languages in which it
contains at least 10,000 nodes.

The largest source of input for ConceptNet is Wiktionary, which provides
18.1 million edges and is mostly responsible for its large multilingual
vocabulary. However, much of the character of ConceptNet comes from OMCS
and the various games with a purpose, which express many different kinds
of relations between terms, such as \emph{PartOf} (``a wheel is part of
a car'') and \emph{UsedFor} (``a car is used for driving'').


\subsection{Relations}\label{relations}

ConceptNet uses a closed class of selected relations such as \emph{IsA},
\emph{UsedFor}, and \emph{CapableOf}, intended to
represent a relationship independently of the language or the source of
the terms it connects.

ConceptNet 5.5 aims to align its knowledge resources on its core set of 36
relations. These generalized relations are similar in purpose to WordNet's
relations such as \emph{hyponym} and \emph{meronym}, as well as to the qualia
of the Generative Lexicon theory \cite{pustejovsky1991generative}.

To allow for expansion to new kinds of knowledge, a data
source can provide new relations that are considered provisional and
appear in a separate namespace. ConceptNet 5.5 includes 10 provisional
relations that come from DBPedia's much larger class of relations,
including \emph{dbpedia/knownFor} and \emph{dbpedia/genre}.

Relations with specific semantics, such as \emph{UsedFor} and
\emph{HasPrerequisite}, tend to connect common words and phrases, while
rarer words are connected by more general relations such as
\emph{Synonym} and \emph{RelatedTo}.

ConceptNet's edges are directed, but as a new feature in ConceptNet 5.5,
some relations are designated as being symmetric: \emph{SimilarTo},
\emph{RelatedTo}, \emph{EtymologicallyRelatedTo}, \emph{Synonym},
\emph{Antonym}, and \emph{DistinctFrom}. The directionality of these
edges is unimportant. The assertion (\emph{A}, \emph{SimilarTo},
\emph{B}) is considered equivalent to (\emph{B}, \emph{SimilarTo},
\emph{A}).


\subsection{Term representation}\label{term-representation}

ConceptNet represents terms in a standardized form. The text is
Unicode-normalized (as NFKC) and lowercased, and split into
non-punctuation tokens using a generalization of the Unicode word
segmentation algorithm. The tokens are joined with underscores, and this
text is prepended with the URI \texttt{/c/lang}, where \emph{lang} is
the BCP 47 language code for the language the term is in. As an example,
the English term ``United States'' becomes
\texttt{/c/en/united\_states}. Relations have a separate namespace of
URIs prefixed with \texttt{/r}, such as \texttt{/r/PartOf}.

The particular tokenizer used is the default tokenizer in the Python
package \texttt{wordfreq}, version 1.5. Its most notable difference from
Unicode tokenization is that it does not insert word breaks between all
Han or Thai characters, whereas the Unicode algorithm would.

% Often, in multilingual code, one must make a choice about how coarsely
% to represent languages, because languages and dialects can have varying
% degrees of mutual intelligibility and partially but not entirely
% overlapping vocabularies. Examples include whether to distinguish
% Simplified from Traditional Chinese, Norwegian Bokm√•l from Nynorsk,
% Bosnian from Croatian from Serbian, and regional variants of Arabic such
% as Egyptian Arabic from Modern Standard Arabic. ConceptNet makes the
% decision to use the coarsest language codes in all of these cases:
% \texttt{zh} for all variants of Chinese, \texttt{no} for Norwegian,
% \texttt{sh} for Serbo-Croatian, \texttt{ar} for Arabic, and so on. The
% motivation is that it's better to include additional terms from a
% related language than to leave gaps in a language's vocabulary. This
% additionally avoids the need to disambiguate data sources that also use
% coarse language codes, such as Serbo-Croatian entries in Wiktionary.

Previous versions of ConceptNet required terms in English to be in lemmatized
form, so that, for example, ``drive'' and ``driving'' would be represented as
the same term, allowing the assertions (\emph{car}, \emph{UsedFor},
\emph{driving}) and (\emph{drive}, \emph{HasPrerequisite}, \emph{have license})
to be connected. With lemmatization, the term ``United States'' was represented
as the confusing-looking \texttt{/c/en/unite\_state}, looking up terms required
ConceptNet-specific preprocessing, and some terms with ambiguous lemmatization
were difficult to look up. ConceptNet 5.5 removes the lemmatizer, and instead
relates inflections of words using the \emph{FormOf} relation. The two
assertions above are now linked by the third assertion (\emph{driving},
\emph{FormOf}, \emph{drive}), and both ``driving'' and ``drive'' can be looked
up in ConceptNet.

One English-specific normalization remains, because English terms are
more likely to come from free text than other languages. A small set of
stopwords is removed from all English terms: ``a'', ``an'', ``the'', and
as the first word of a term, ``to''. Removing other stopwords is left up
to the importer for a particular data source.


\subsection{Vocabulary}\label{vocabulary}

When building a knowledge graph, the decision of what a node should
represent has significant effects on how the graph is used. It also has
implications that can make linking and importing other resources
non-trivial, because different resources make different decisions about
their representation.

In ConceptNet, a node is a word or phrase of a natural language, often a
common word in its undisambiguated form. The word ``lead'' in English is
a term in ConceptNet, represented by the URI \texttt{/c/en/lead}, even
though it has multiple meanings. The advantage of ambiguous terms is
that they can be extracted easily from natural language, which is also
ambiguous. This representation is equivalent to that used by systems
that learn distributional semantics from text, such as word2vec
\cite{mikolov2013word2vec}.

ConceptNet imports knowledge from some other systems, but this usually
is not as straightforward as, for example, simply including WordNet as a
subgraph of ConceptNet. These other systems have their own target
vocabularies that need to be aligned with ConceptNet, which is a
somewhat lossy operation. WordNet's nodes are synsets -- sets of senses
of words that are assumed to be interchangeable, even though they may
have different connotations. DBPedia's nodes are titles of encyclopedia
articles, which are generally either specific named entities or topic
areas such as ``Astronomy''. OpenCyc's nodes are a set of logical
predicates, with heuristically-generated mappings to natural language
phrases. All of these have an underspecified many-to-many mapping to
ConceptNet terms, and importing them thus requires heuristics to
transform them into ConceptNet's vocabulary.

A term that is imported from another knowledge graph will be connected
to ConceptNet nodes via the \texttt{ExternalURL} ConceptNet relation.
(This is similar in spirit to the OWL \texttt{sameAs} relation, but
\texttt{sameAs} is not suited for a many-to-many mapping because it is
supposed to be transitive.) This preserves the provenance of the data
and enables looking up what the untransformed data was, and also allows
ConceptNet to participate in the broader ecosystem of Linked Open Data.

\subsection{Word senses}\label{word-senses}

ConceptNet's representation allows for more specific, disambiguated
versions of a term. The URI \texttt{/c/en/lead/n} refers to noun senses
of the word ``lead'', and is effectively included within
\texttt{/c/en/lead} when searching or traversing ConceptNet, and
linked to it with the implicit relation \emph{SenseOf}. Many data
sources provide information about parts of speech, allowing us to use
this as a common representation that provides a small amount of
disambiguation.

We have experimented with further disambiguating individual word senses,
as in \texttt{/c/en/lead/n/metal}, following up on work that showed that
word senses can be disambiguated by finding the best match to nearby
words on features including ConceptNet \cite{havasi2010coarse}.

However, we have not found a satisfying way to include a vocabulary of
word senses in ConceptNet, as there is no currently standardized
vocabulary of word senses. Resources that distinguish word senses, such
as WordNet, Wiktionary, and to some extent DBPedia, each use their own
resource-specific vocabulary of word senses. Mappings between these
various vocabularies exist \cite{miller2014alignment}, but they are a
topic of ongoing research which has not produced a stable target for
word sense disambiguation.

In ConceptNet, we tried including these
disambiguations in resource-specific namespaces (such as
\texttt{/c/en/lead/n/wp/electronics} for the sense of ``lead''
disambiguated as ``Electronics'' on the English Wikipedia), which would
be linked to their parent terms by \emph{SenseOf} relations. We
ultimately chose not to include these disambiguations in ConceptNet
because they decreased its performance on intrinsic evaluations of word
similarity. The confusion created by undisambiguated words was not as
bad as the sparsity created by unaligned word senses. So we have settled
on disambiguating only the part of speech, which is also the representation
of word senses targeted by the sense2vec system \cite{trask2015sense2vec}.


\subsection{Tracking the provenance of knowledge}\label{provenance}

Each assertion contains extra data for tracking the provenance of the
knowledge it contains, which is used to find systematic errors and bad
contributions. Sources are represented by URIs and divided into multiple
types. The prefix \texttt{/s/contributor} represents individual people
who contributed knowledge to OMCS or one of its sister projects.
\texttt{/s/activity} represents what a contributor was doing when they
contributed the knowledge, such as responding to a prompt or playing a
game with a purpose. \texttt{/s/resource} represents an existing
knowledge resource, such as WordNet or Wiktionary. \texttt{/s/process}
represents a process for transforming knowledge, such as the Wiktionary
parser or a rule for fixing the parsing of prepositions in OMCS.

The provenance of an assertion is a set of sets of sources. The inner
sets indicate multiple sources that were used together to create the
assertion: for example, a contributor, an activity, and a process may
all be involved in collecting one assertion. Each inner set of sources
contributes some amount of weight to the assertion. This number is
provided by the import scripts for each knowledge source, which are
asked to estimate the reliability of each assertion they produce.

The outer set represents sources that separately provided the same
assertion. When there are multiple separate sources, they add their
weights together, indicating that the assertion is more reliable because
we know it for multiple reasons.


% \subsection{The use-mention distinction}

% ConceptNet does not strictly enforce the use-mention distinction. Some
% of its assertions, such as (\emph{cat}, \emph{IsA}, \emph{animal}),
% relate the meanings of the words as they are used, while others, such as
% (\emph{cat}, \emph{DerivedFrom}, \emph{cattus}), mention the words as
% objects in themselves. However, it is intended that all relations,
% including these ``mention'' relations, informatively represent something
% about the meanings of the words. The etymology of a word, as indicated
% by the \emph{DerivedFrom} or \emph{EtymologicallyRelatedTo} relations,
% is often a clue to its meaning.

% Unacceptable assertions would include (\emph{cat}, \emph{HasA},
% \emph{three letters}) or (\emph{noon}, \emph{IsA}, \emph{palindrome}).
% Sometimes these appear in data sources, particularly games with a
% purpose, whose players are tempted to use wordplay to achieve their
% goals. The code that imports data from the word game Verbosity
% heuristically detects wordplay, and mentions of words as words, so that
% it can filter them out.

\section{Related work}

% WordNet
% Freebase / Google Knowledge Graph
% DBPedia
% Retrofitting
% word2vec
% GloVe
% Levy, Hyperwords


\section{Applying ConceptNet to Word Embeddings}
\label{applying-conceptnet-to-word-embeddings}

Word embeddings represent words as low-dimensional vectors of real numbers,
where vectors that are close together are semantically related. Recent
developments in training word embeddings have yielded state-of-the-art results
on many NLP tasks. In this section, we show how to effectively generate
word embeddings, and improve existing word embeddings using ConceptNet.

\subsection{Building word embeddings from ConceptNet}

We represent the ConceptNet graph as a sparse, symmetric term-term matrix $M$.
Each cell contains the sum of the edge weights between the two corresponding
terms. For performance reasons, we discard terms connected to fewer than three
edges. The discarded terms will later be reintroduced as the weighted average of
their related terms.

To measure the association between terms, we treat $M$ as a co-occurrence
matrix and approximate the positive pointwise mutual information (PPMI) between
the terms. In addition, we apply context distributional smoothing with
$\alpha=0.75$ \cite{levy2015embeddings}. This helps discount rare words. The
resulting matrix $M^\text{PPMI}$ can be computed as:

$$
M^\text{PPMI}_{ij} =
\log \frac
  {\hat{P} \left(i, j\right)}
  {\hat{P} \left(i\right) \hat{P_\alpha} \left(j\right)}
$$
$$
\hat{P_\alpha} \left(j\right) =
\frac
  {\left(\sum_i{M_{ij}}\right)^{0.75}}
  {\sum_k{\left(\sum_i{M_{ik}}\right)^{0.75}}}
$$

We generate a dense representation using a truncated singular value
decomposition (SVD). An SVD computes the optimal $d$-dimensional word embedding,
in terms $l2$ reconstruction loss. For a given matrix $M$, it returns the
decomposition

$$
M = USV^T
$$

where $U$ and $V$ are orthogonal, and $S$ is diagonal. A Truncated SVD only
computes the columns of $U$ and the rows of $V$ that correspond to the most
significant singular values. Here, we select $d=300$.

Finally, we obtain the desired word embeddings by $l1$-normalizing the columns
of $U$ and $V$, then adding them. Adding $U$ and $V$ together includes second
order similarity terms into the vectors. $l1$-normalizing the columns can be
seen as weighing each dimension by their sparsity.

\subsection{Combining ConceptNet with distributional word embeddings}
% Expanded retrofitting and redecomposition

\section{Evaluation methods}

[...]

To compare the word-embedding ensemble presented in this paper (ConceptNet
Numberbatch) to other systems, we will first compare results on intrinsic
evaluations of the word embeddings themselves, involving word relatedness and
proportional analogies. We will also use an extrinsic evaluation of story
understanding, showing that better embeddings lead to better performance on a
linguistic task.

\subsection{Intrinsic evaluations of word relatedness}
\label{intrinsic-evaluations}
% Word relatedness and analogy evaluations

One way to evaluate the intrinsic performance of a semantic space is to ask it
to rank the relatedness of pairs of words, and compare its judgments to human
judgments.\footnote{It is sometimes important to distinguish \emph{similarity}
from \emph{relatedness}. For example, the term ``coffee'' is related to
``mug'', but coffee is not \emph{similar} to a mug. What a machine can learn
from the connectivity of ConceptNet is focused on relatedness.} If one word in
a pair is out-of-vocabulary, the pair is assumed to have a relatedness of 0. A
good semantic space will provide a ranking of relatedness that is highly
correlated with the human gold-standard ranking, as measured by its Spearman
correlation ($\rho$).

Many gold standards of word relatedness are in common use. Here, we focus on
MEN-3000 \cite{bruni2014men}, a large crowd-sourced ranking of common words; RW
\cite{luong2013rw}, a ranking of rare words; and MTurk-771
\cite{halawi2012mturk}, another crowd-sourced evaluation of a variety of words.
To avoid manually overfitting by designing our semantic space around a
particular evaluation, we experimented using smaller development sets, holding
out some test data until it was time to include results in this paper:

\begin{itemize}
\item
    MEN-3000 is already divided into a 2000-item development set and a
    1000-item test set. We use the results from the test set as the final results.
\item
    RW has no standard dev/test breakdown. We sampled 2/3 of its items as
    a development set and held out the other 1/3 (every third line of the file,
    starting with the third). However, we report results on the complete set,
    for consistency with other publications.
\item
    We did not use MTurk-771 in development, holding out the entire set
    as a final test, showing that ConceptNet Numberbatch performs well on a
    previously-unseen vocabulary.
\end{itemize}


[...]

Previous work that combines ConceptNet in an ensemble with other word
embeddings \cite{speer2016ensemble} has included a time-consuming step of
unifying the vocabularies of multiple resources and interpolating what the
missing word embeddings should be. With ConceptNet 5.5, we found this
unnecessary.  The vocabulary of ConceptNet 5.5 is sufficient to cover all but
four words that appear anywhere in the English test data.\footnote{We are
content to return zero-vectors for the four out-of-vocabulary words, which are
``amazings'', ``internationaler'', ``liverpools'', and ``discoverys''. All of
these are from the Rare Words evaluation, and are probably the result of
imperfect machine-generation of examples.}

\subsection{Intrinsic evaluations of analogies}

Proportional analogies are statements of the form ``$a_1$ is to $b_1$ as $a_2$
is to $b_2$''. The task of filling in missing values of a proportional analogy
was common until recently on standardized tests such as the SAT. Now, it is
popular as a way to show that a semantic space can approximate relationships
between words, even without taking explicit relationships into account.

Much of the groundwork for evaluating systems' ability to solve proportional
analogies was laid by Peter Turney, including his method of Latent Relational
Analysis \cite{turney2005lra}, which was quite effective at solving
proportional analogies by repeatedly searching the Web for the words involved
in them.  A newer method called SuperSim \cite{turney2013distributional} does
not require Web searching. These methods are evaluated on a dataset of 374 SAT
questions that Turney and his collaborators have collected, which they provide
on request.  Unfortunately, the questions are not licensed for free
distribution.

[... describe our pairwise analogy heuristic ...]

Many systems now are instead evaluated on a synthetic dataset that was packaged
with Google's word2vec, which tests a much smaller variety of analogies focused
on syntactic changes to words, gendered words, and geographic facts. The
appropriateness of this dataset for measuring vector spaces has been questioned
\cite{linzen2016issues}, based on the variability of the results and the fact
that many of the analogies can be solved while ignoring one or more terms in
the query.

We will evaluate our system on the Google analogies simply to allow comparisons
to other systems that focus on this evaluation. Levy showed that comparing word
similarities with the 3CosMul heuristic \cite{levy2015embeddings} performs best
on this data, and made his results easily reproducible and comparable by
providing evaluation software called Hyperwords. We will use Hyperwords in this
paper to compare systems on the Google analogy task, as well as the purely
syntactic MSR task.

The Google and MSR evaluations test whether a system returns the single word
deemed to be the correct completion of an analogy, out of its entire
vocabulary. The results are thus sensitive to the size of the vocabulary, and
benefit from a limited vocabulary that includes enough of the prompt words.
When exporting ConceptNet Numberbatch to the Hyperwords format, we select the
200,000 most common English words, according to the \emph{wordfreq} data set
\cite{speer2016wordfreq}, that appear in the pruned vocabulary of ConceptNet
(see ``Building word embeddings from ConceptNet'').

\subsection{An extrinsic evaluation of story understanding}
\label{extrinsic-evaluation}
% Story Cloze Test


% Paragraphs that were cut, but we might include if there's space:
%
% WordNet has a fine-grained vocabulary of word senses that are
% distinguished by the synsets that they participate in. Each language of
% Wiktionary has its own vocabulary of word senses; the English Wiktionary
% actually has two different word sense vocabularies, one used for
% definitions and one used for translations. Wikipedia disambiguation
% pages are another vocabulary of word senses that can be applied to
% resources derived from it, such as DBPedia. Granularity can make
% vocabularies difficult to align: the English word ``run'' has 57 senses
% in WordNet, 17 in the French Wiktionary, and 4 in the German Wiktionary.
% The English Wiktionary has 45 senses of ``run'' that are used in
% translations, which map in an underspecified way to at least 96
% definitions.

% Sometimes this alignment involves using extra information that
% incidentally comes with the data. To include WordNet, we need to assign
% each synset a primary label that will be used in its semantic relations,
% and then link other synonyms in the set to that label using the
% \emph{Synonym} relation. In WordNet RDF 3.1, the labels are unordered,
% although some terms are more appropriate as primary labels than others.
% Fortunately, WordNet RDF 2.0, which is linked to the current WordNet RDF
% 3.1 with \texttt{owl\#SameAs} links, also assigned primary labels to
% synsets by putting these labels in their URIs. So when the synset at
% http://wordnet-rdf.princeton.edu/wn31/100028005-n is linked to
% http://www.w3.org/2006/03/wn/wn20/instances/synset-shape-noun-2, we
% conclude that its primary label is ``shape''. This is one of many
% heuristics that are tried when importing WordNet synsets into
% ConceptNet.

\bibliographystyle{aaai}
\bibliography{conceptnet-paper}

\end{document}
